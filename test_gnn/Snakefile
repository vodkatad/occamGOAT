import utils
rule check_GPU:
    run:
        import torch
        print(torch.cuda.is_available())

rule py:
    run: 
        import sys
        print(sys.path)

rule run_gnn:
    output:out='train_test_stat.csv'
    run:
            from torch_geometric.datasets import TUDataset
            import torch
            from torch.nn import Linear
            import torch.nn.functional as F
            from torch_geometric.nn import GCNConv
            from torch_geometric.nn import global_mean_pool
            from torch_geometric.loader import DataLoader
            from utils import GCN
            import pandas as pd

            dataset = TUDataset(root='data/TUDataset', name='MUTAG')

            print()
            print(f'Dataset: {dataset}:')
            print('====================')
            print(f'Number of graphs: {len(dataset)}')
            print(f'Number of features: {dataset.num_features}')
            print(f'Number of classes: {dataset.num_classes}')

            data = dataset[0]  # Get the first graph object

            print()
            print(data)
            print('=============================================================')

            # print some statistics about the first graph.
            print(f'Number of nodes: {data.num_nodes}')
            print(f'Number of edges: {data.num_edges}')
            print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')
            print(f'Has isolated nodes: {data.has_isolated_nodes()}')
            print(f'Has self-loops: {data.has_self_loops()}')
            print(f'Is undirected: {data.is_undirected()}')
            torch.manual_seed(42)
            dataset = dataset.shuffle()

            train_dataset = dataset[:150]
            test_dataset = dataset[150:]

            print(f'Number of training graphs: {len(train_dataset)}')
            print(f'Number of test graphs: {len(test_dataset)}')
            

            train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
            test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
            embedding_dim=dataset.num_features
            out_dim=dataset.num_classes
            model = GCN(embedding_dim=embedding_dim,hidden_channels=64,out_dim=out_dim)
            print(model)

            optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
            criterion = torch.nn.CrossEntropyLoss()

            def train(t_loader):
                model.train()

                for data in t_loader:  # Iterate in batches over the training dataset.
                    out = model(data.x, data.edge_index, data.batch)  # forward pass.
                    loss = criterion(out, data.y)  # Compute the loss
                    loss.backward()  # propagate loss/derive gradients
                    optimizer.step()  # update weights in the net based on gradients.
                    optimizer.zero_grad()  # Clear gradient

            def test(loader):
                model.eval()

                correct = 0
                for data in loader:  # Iterate in batches over the training/test dataset
                    out = model(data.x, data.edge_index, data.batch)  
                    pred = out.argmax(dim=1)  # get prediction as max probability
                    correct += int((pred == data.y).sum())  # calculate good precitions
                return correct / len(loader.dataset)  # Derive ratio of correct predictions

            stat=pd.DataFrame(columns=['Epoch', 'Train_acc', 'Test_acc'])
            for epoch in range(1, 171):
                train(train_loader)
                train_acc = test(train_loader)
                test_acc = test(test_loader)
                new={'Epoch':epoch,'Train_acc':train_acc,'Test_acc':test_acc}
                stat.loc[len(stat)]=new
                print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')
            stat.to_csv(output.out,sep=',',index=False)
